{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# MRI Super\u2011Resolution \u2013 Unified Workflow Notebook\n\nThis notebook replaces all individual CLI scripts with a **single, configuration\u2011driven workflow** covering:\n\n1. **Setup & Configuration**  \n2. **Data Exploration**  \n3. **Model Training**  \n4. **Evaluation**  \n5. **Inference / Visualisation**\n\nEdit YAML files in `configs/` to modify parameters \u2014 the notebook simply *loads* them and passes a frozen `CfgNode` around.\n\n> **Tip:** Run each section independently; the state (e.g.\\ `trainer`, `evaluator`, `model`) is stored in top\u2011level variables so you can jump around without rerunning everything."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# --- 1. Setup -----------------------------------------------------------------\nimport os, sys, yaml, torch, importlib, pprint, json\nfrom pathlib import Path\n\n# Make src importable\nPROJECT_ROOT = Path().resolve()\nSRC_DIR = PROJECT_ROOT / \"src\"\nif str(SRC_DIR) not in sys.path:\n    sys.path.insert(0, str(SRC_DIR))\n\nfrom utils.config import load_cfg   # new helper we'll add to utils\ncfg_path = PROJECT_ROOT / \"configs\" / \"experiment.yaml\"\nCFG = load_cfg(cfg_path)\n\nprint(\"Loaded configuration:\")\npprint.pprint(CFG)\nprint(f\"Running on device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Why a YAML\u2011only config?\n\n* **Single source of truth** \u2013 no hidden defaults scattered across scripts.  \n* **Reproducibility** \u2013 every experiment can be re\u2011run by pointing to the same YAML.  \n* **Composability** \u2013 configs can inherit from a *base* config and override only what changes."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# --- 2. Data Exploration ------------------------------------------------------\nfrom utils.data import create_dataloaders\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntrain_loader, val_loader = create_dataloaders(\n    data_dir=CFG['data']['dir'],\n    config_path=CFG['data']['transforms'],\n    loader_to_create='both',\n    batch_size=1,\n    num_workers=CFG['data'].get('workers', 4)\n)\n\ndef show_sample(loader, title):\n    lr, hr = next(iter(loader))\n    plt.figure(figsize=(6,3))\n    plt.subplot(1,2,1); plt.imshow(lr[0,0], cmap='gray'); plt.title('LR'); plt.axis('off')\n    plt.subplot(1,2,2); plt.imshow(hr[0,0], cmap='gray'); plt.title('HR'); plt.axis('off')\n    plt.suptitle(title); plt.show()\n\nshow_sample(train_loader, \"Training sample\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# --- 3. Model Training --------------------------------------------------------\nfrom train_refactored import Trainer\ntrainer = Trainer(  # modified Trainer now accepts a Namespace\u2011like dict\n    argparse.Namespace(\n        **CFG['train']  # spread config keys as if they were CLI args\n    )\n)\ntrainer.fit()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# --- 4. Evaluation ------------------------------------------------------------\nfrom evaluate_refactored import Evaluator\nevaluator = Evaluator(argparse.Namespace(\n    **CFG['eval'],\n    checkpoint=str(trainer.output_dir / f\"{CFG['train']['model']}_best.pth\")\n))\nmetrics = evaluator.run()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# --- 5. Inference -------------------------------------------------------------\nfrom inference import preprocess_image\nfrom PIL import Image\nimport torch.nn.functional as F\nimport numpy as np\n\ntest_img_path = list(Path(CFG['inference']['input_dir']).glob('*.tif'))[0]\nlr = preprocess_image(test_img_path).to(evaluator.device)\nlr_up = F.interpolate(lr, scale_factor=2, mode='bicubic', align_corners=False)\nwith torch.no_grad():\n    sr = evaluator.model(lr_up)\n\n# Visualise\nfrom utils.visualization import plot_comparison\nplot_comparison(lr_up.cpu(), sr.cpu(), sr.cpu())  # placeholder HR = SR for demo"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}